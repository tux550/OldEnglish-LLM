{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Iterable\n",
    "from huggingface_hub import notebook_login, Repository, HfApi\n",
    "from datasets import Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to huggingface\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face config\n",
    "HF_USERNAME = 'RodrigoSalazar-U'\n",
    "HF_REPO = 'ANG-dataset'\n",
    "HF_REPO_PATH = \"{}/{}\".format(HF_USERNAME, HF_REPO)\n",
    "\n",
    "# Splits config\n",
    "SPLIT_TRAIN_RATIO = 0.7\n",
    "SPLIT_TEST_RATIO = 0.2\n",
    "SPLIT_VAL_RATIO = 0.1\n",
    "\n",
    "# Pretraining config\n",
    "PRETRAINING_COD_RATE = 0.5\n",
    "\n",
    "# Random seeds\n",
    "RANDOM_SEEDS = {\n",
    "  \"split0\" : 68,\n",
    "  \"split1\" : 41,\n",
    "  \"split2\" : 53,\n",
    "  \"cod0\": 35,\n",
    "  \"corpus0\": 7\n",
    "}\n",
    "\n",
    "### === FILE LOCATIONS === ###\n",
    "FILES_ROOT=\"data\"\n",
    "FILES_TRANSLATE_FRAGS=f\"{FILES_ROOT}/TranslatedByFragment.tab\"\n",
    "FILES_TRANSLATE_WORDS=f\"{FILES_ROOT}/TranslatedByWord.tab\"\n",
    "FILES_DOEC=f\"{FILES_ROOT}/DOECByFragmentTextOnly.txt\"\n",
    "\n",
    "\n",
    "\n",
    "### === CONSTANTS === ###\n",
    "LANG_CODE_ANG = \"ANG\"\n",
    "LANG_CODE_EN = \"EN\"\n",
    "COL_ANG_TEXT = \"ANG_text\"\n",
    "COL_EN_TEXT = \"EN_text\"\n",
    "COL_ANG_WORD = \"ANG_word\"\n",
    "COL_EN_WORD = \"EN_word\"\n",
    "COL_EN_DEF = \"EN_def\"\n",
    "COL_COD_EN2ANG = \"COD_EN2ANG\"\n",
    "COL_COD_ANG2EN = \"COD_ANG2EN\"\n",
    "\n",
    "# Suppress specific warning message\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### === PRINT UTILS === ###\n",
    "def getDatasetStats(df_dict : dict[str,pd.DataFrame]):\n",
    "    total_size = 0\n",
    "    for df in df_dict.values():\n",
    "        total_size += len(df)\n",
    "    # Create temporary dataframe with stats\n",
    "    tmp_df = []\n",
    "    for name, df in df_dict.items():\n",
    "        tmp_df.append({\"name\": name, \"size\": len(df), \"percentage\": 100 * len(df)/total_size, \"columns\": \", \".join(sorted(df.columns))})\n",
    "    # Display stats\n",
    "    tmp_df = pd.DataFrame(tmp_df)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "WORD_TOKENIZER = PunktLanguageVars()\n",
    "\n",
    "NORMALIZE_ANG = {\n",
    "    # OLD ENGLISH CHARACTERS\n",
    "    ## Reference: from cltk.phonology.ang.transcription import Word\n",
    "    ## These characters are not present in Modern English\n",
    "    ## If we desired to standarize them to ASCII, we would use the following mapping\n",
    "    ## Nonetheless, experimental results shows better performance when keeping these characters\n",
    "    ## It is hypothesized that it allows the model to better differentiate between Old English and Modern English\n",
    "    #\"Æ\": \"Ae\",\n",
    "    #\"æ\": \"ae\",\n",
    "    #\"Ƿ\": \"W\",\n",
    "    #\"ƿ\": \"w\",\n",
    "    #\"Þ\": \"Th\",\n",
    "    #\"þ\": \"th\",\n",
    "    #\"Ð\": \"D\", \n",
    "    #\"ð\": \"d\",  \n",
    "\n",
    "    # DIACRITICS\n",
    "    ## Remove diacritics identified in the DOEC\n",
    "    \"é\": \"e\",\n",
    "    \"á\": \"a\",\n",
    "    \"â\": \"a\",\n",
    "    \"í\": \"i\",\n",
    "    \"î\": \"i\",\n",
    "    \"ó\": \"o\",\n",
    "    \"ô\": \"o\",\n",
    "    \"ú\": \"u\",\n",
    "    \"û\": \"u\",\n",
    "    \"ý\": \"y\",\n",
    "    \"è\": \"e\",\n",
    "    \"ê\": \"e\",\n",
    "    \"ř\": \"r\",\n",
    "    \n",
    "    # SPECIAL CHARACTERS\n",
    "    ## Remove special characters identified in the DOEC\n",
    "    \"∂\": \"d\",\n",
    "    \"ω\": \"w\",\n",
    "    \"œ\": \"oe\",\n",
    "    \"đ\": \"d\",\n",
    "}\n",
    "\n",
    "def word_tokenize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \"\"\"\n",
    "    return WORD_TOKENIZER.word_tokenize(text)\n",
    "\n",
    "def standarize_ang(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standarize Old English text using a set of rules.\n",
    "    \"\"\"\n",
    "    for char, repl in NORMALIZE_ANG.items():\n",
    "        text = text.replace(char, repl)\n",
    "    return text\n",
    "\n",
    "def standarize_raw(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standarize raw text by lowercasing and removing special characters.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Split into tokens\n",
    "    tokens = WORD_TOKENIZER.word_tokenize(text)\n",
    "    # Join tokens\n",
    "    text = \" \".join(tokens)\n",
    "    # Standarize Old English\n",
    "    text = standarize_ang(text)\n",
    "    # Special unicode characters\n",
    "    text = text.replace(\"­\",\"\")\n",
    "    text = text.replace(\" ­–\",\"-\")\n",
    "    # Remove whitespaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    # Standarize punctuation\n",
    "    text = text.replace(\"—\", \"-\")\n",
    "    text = text.replace(\"–\", \"-\")\n",
    "    text = text.replace(\"“\", \"\\\"\")\n",
    "    text = text.replace(\"”\", \"\\\"\")\n",
    "    text = text.replace(\"‘\", \"'\")\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"·\", \".\")\n",
    "    text = text.replace(\"´\", \"'\")\n",
    "    text = text.replace(\"¸\", \"'\")\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "The dataset is compiled from the following sources:\n",
    "\n",
    "- [Dictionary of Old English Corpus (DOEC)](https://varieng.helsinki.fi/CoRD/corpora/DOEC/)  \n",
    "  A collection of approximately 3 million words of Old English texts dating from 600–1150 AD. The text is not annotated.\n",
    "\n",
    "- **Translated DOEC**  \n",
    "  A subset of the DOEC corpus that includes translations into Modern English. This serves as a parallel corpus at both the word and sentence levels.\n",
    "\n",
    "- [Bosworth-Toller Anglo-Saxon Dictionary](http://www.bosworthtoller.com/)  \n",
    "  A comprehensive dictionary of Old English words and their meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes use of the definitions view. This view is created by the following SQL query:\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE VIEW definitions_view AS\n",
    "SELECT \n",
    "    e.normalized AS word,\n",
    "    GROUP_CONCAT(DISTINCT wc.name ORDER BY wc.name SEPARATOR '; ') AS wordclass,\n",
    "    GROUP_CONCAT(DISTINCT wca.name ORDER BY wca.name SEPARATOR '; ') AS wordcategory,\n",
    "    GROUP_CONCAT(DISTINCT d.normalized ORDER BY d.normalized SEPARATOR '; ') AS definitions\n",
    "FROM \n",
    "    entries e\n",
    "-- Wordclasses and wordcategories\n",
    "LEFT JOIN\n",
    "    (\n",
    "        SELECT \n",
    "            entries_wordclass.entry_id,\n",
    "            wordclasses.name\n",
    "        FROM \n",
    "            entries_wordclass\n",
    "        JOIN\n",
    "            wordclasses ON entries_wordclass.wordclass_id = wordclasses.id\n",
    "    ) wc\n",
    "    ON e.id = wc.entry_id\n",
    "LEFT JOIN\n",
    "    (\n",
    "        SELECT \n",
    "            entries_wordcategory.entry_id,\n",
    "            wordcategories.name\n",
    "        FROM \n",
    "            entries_wordcategory\n",
    "        JOIN\n",
    "            wordcategories ON entries_wordcategory.wordcategory_id = wordcategories.id\n",
    "    ) wca\n",
    "    ON e.id = wca.entry_id\n",
    "-- Include definitions (MUST exist)\n",
    "JOIN \n",
    "    definitions d ON e.id = d.entry_id\n",
    "WHERE\n",
    "    -- Filter out prefixes and suffixes\n",
    "    e.is_prefix = False\n",
    "    AND\n",
    "    e.is_suffix = False\n",
    "    AND \n",
    "    -- Filter out tiny words. Usually these are hard to replace\n",
    "    LENGTH(e.orthographic) > 2\n",
    "    AND\n",
    "    -- Either wordclass or wordcategory must be present\n",
    "    (wc.name IS NOT NULL OR wca.name IS NOT NULL)\n",
    "GROUP BY \n",
    "    e.normalized\n",
    "HAVING\n",
    "    -- Only include words with a single wordclass and wordcategory\n",
    "    COUNT(DISTINCT wc.name) = 1\n",
    "    AND\n",
    "    COUNT(DISTINCT wca.name) = 1\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_defintions(\n",
    "    db_uri: str = \"localhost\",\n",
    "    db_user: str = \"root\",\n",
    "    db_pass: str = \"admin\",\n",
    "    db_database: str = \"dictionary\",\n",
    "    db_view: str = \"definitions_view\",\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Read dictionary data from MySQL database.\n",
    "    \"\"\"\n",
    "    db = mysql.connector.connect(\n",
    "        host=db_uri,\n",
    "        user=db_user,\n",
    "        password=db_pass,\n",
    "        database=db_database\n",
    "    )\n",
    "    # Get only uniquely identifiable words\n",
    "    df : pd.DataFrame = pd.read_sql(f\"SELECT * FROM {db_view}\", db) # type: ignore\n",
    "    df = df[[\"word\", \"definitions\", \"wordclass\", \"wordcategory\"]] \n",
    "    # Rename columns\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"word\": COL_ANG_WORD,\n",
    "            \"definitions\": COL_EN_DEF\n",
    "        })\n",
    "    # Apply standarization\n",
    "    df[COL_ANG_WORD] = df[COL_ANG_WORD].apply(standarize_raw)\n",
    "    # Index using ANG WORD\n",
    "    df = df.set_index(COL_ANG_WORD)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_parallel(\n",
    "  filepath: str,\n",
    "  headers: list = [\"text\", \"translation\"],\n",
    "  headers_mapping: dict = {\n",
    "      \"text\": COL_ANG_TEXT,\n",
    "      \"translation\": COL_EN_TEXT\n",
    "  }) -> pd.DataFrame:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "      df = pd.read_csv(f, sep=\"\\t\", header=None, names=headers, quoting=csv.QUOTE_NONE)\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    # Standarize the text and translation columns\n",
    "    for header in headers:\n",
    "        df[header] = df[header].apply(standarize_raw)\n",
    "    # Rename columns to text=ANG and translation=EN\n",
    "    df = df.rename(columns=headers_mapping)\n",
    "    # Drop any empty rows\n",
    "    df = df[(df[COL_ANG_TEXT] != \"\") & (df[COL_EN_TEXT] != \"\")]\n",
    "    df = df[(df[COL_ANG_TEXT] != \" \") & (df[COL_EN_TEXT] != \" \")]\n",
    "    return df\n",
    "\n",
    "def load_parallel_words(\n",
    "  filepath: str,\n",
    "  headers: list = [\"word\", \"translation\", \"text\", \"translation_text\"],\n",
    "  headers_mapping: dict = {\n",
    "      \"word\": COL_ANG_WORD,\n",
    "      \"translation\": COL_EN_WORD,\n",
    "      \"text\": COL_ANG_TEXT,\n",
    "      \"translation_text\": COL_EN_TEXT\n",
    "  }):\n",
    "  with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    df = pd.read_csv(f, sep=\"\\t\", header=None, names=headers, quoting=csv.QUOTE_NONE)\n",
    "  # Drop rows with NaN values\n",
    "  df = df.dropna()\n",
    "  # Standarize the text and translation columns\n",
    "  for header in headers:\n",
    "      df[header] = df[header].apply(standarize_raw)\n",
    "      # Drop any empty rows\n",
    "      df = df[df[header] != \"\"]\n",
    "      df = df[df[header] != \" \"]\n",
    "  # Rename columns to text=ANG and translation=EN\n",
    "  df = df.rename(columns=headers_mapping)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "      filepath: str,\n",
    "      header: str = \"text\"\n",
    "  ):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(lines, columns=[header])\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    # apply standarization\n",
    "    df[header] = df[header].apply(standarize_raw)\n",
    "    # Drop any empty rows\n",
    "    df = df[df[header] != \"\"]\n",
    "    df = df[df[header] != \" \"]\n",
    "    # Rename to ANG\n",
    "    df = df.rename(columns={header: COL_ANG_TEXT})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dictionary...\n",
      "Reading translation files...\n",
      "Reading DOEC files...\n",
      "Done reading files.\n",
      "========= Initial datasets =========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>percentage</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dictionary</td>\n",
       "      <td>8929</td>\n",
       "      <td>4.209727</td>\n",
       "      <td>EN_def, wordcategory, wordclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Translate</td>\n",
       "      <td>14358</td>\n",
       "      <td>6.769321</td>\n",
       "      <td>ANG_text, EN_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOEC</td>\n",
       "      <td>188817</td>\n",
       "      <td>89.020952</td>\n",
       "      <td>ANG_text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    size  percentage                          columns\n",
       "0  Dictionary    8929    4.209727  EN_def, wordcategory, wordclass\n",
       "1   Translate   14358    6.769321                ANG_text, EN_text\n",
       "2        DOEC  188817   89.020952                         ANG_text"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# READ FILES\n",
    "print(\"Reading dictionary...\")\n",
    "dictionary_df = load_defintions()\n",
    "print(\"Reading translation files...\")\n",
    "translate_df = load_parallel(FILES_TRANSLATE_FRAGS)\n",
    "word_translate_df = load_parallel_words(FILES_TRANSLATE_WORDS)\n",
    "print(\"Reading DOEC files...\")\n",
    "doec_df = load_corpus(FILES_DOEC)\n",
    "print(\"Done reading files.\")\n",
    "\n",
    "# Show stats\n",
    "print(\"========= Initial datasets =========\")\n",
    "getDatasetStats({\"Dictionary\": dictionary_df, \"Translate\": translate_df, \"DOEC\": doec_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, test\n",
    "def split_ttv(df: pd.DataFrame, train_size: float, val_size: float, test_size: float, random_state: int):\n",
    "  # Get random seeds\n",
    "  random.seed(random_state)\n",
    "  seed_1 = random.randint(0, 1000)\n",
    "  seed_2 = random.randint(0, 1000)\n",
    "\n",
    "  # Split the dataset\n",
    "  train_df, test_df = train_test_split(df, test_size=(val_size + test_size), random_state=seed_1)\n",
    "  val_df, test_df = train_test_split(test_df, test_size=test_size/(val_size + test_size), random_state=seed_2)\n",
    "  return train_df, val_df, test_df\n",
    "\n",
    "def split_tt(df: pd.DataFrame, train_size: float, random_state: int):\n",
    "  # Split the dataset\n",
    "  train_df, test_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Translate datasets =========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>percentage</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>10050</td>\n",
       "      <td>69.995821</td>\n",
       "      <td>ANG_text, EN_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test</td>\n",
       "      <td>2872</td>\n",
       "      <td>20.002786</td>\n",
       "      <td>ANG_text, EN_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Val</td>\n",
       "      <td>1436</td>\n",
       "      <td>10.001393</td>\n",
       "      <td>ANG_text, EN_text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name   size  percentage            columns\n",
       "0  Train  10050   69.995821  ANG_text, EN_text\n",
       "1   Test   2872   20.002786  ANG_text, EN_text\n",
       "2    Val   1436   10.001393  ANG_text, EN_text"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split translate dataset\n",
    "translate_train_df, translate_test_df, translate_val_df = split_ttv(\n",
    "  translate_df,\n",
    "  SPLIT_TRAIN_RATIO,\n",
    "  SPLIT_TEST_RATIO,\n",
    "  SPLIT_VAL_RATIO,\n",
    "  RANDOM_SEEDS[\"split0\"]\n",
    ")\n",
    "\n",
    "# Print stats\n",
    "print(\"========= Translate datasets =========\")\n",
    "getDatasetStats({\"Train\": translate_train_df, \"Test\": translate_test_df, \"Val\": translate_val_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Enrichment\n",
    "\n",
    "To smooth the model's learning process, we will enrich the dataset with the Chain-of-Dictionary (CoD) technique. This technique consists of incorporating word-level translations into the fragments of the DOEC corpus that have been translated into Modern English. In orther to help tackle the problem of words Out-of-Vocabulary (OOV) the technique used for sampling the words is based on their frequency in the corpus, giving more weight to the less frequent words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### === FREQUENCY UTILS === ###\n",
    "def word_frecuency(dataset: Iterable[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the frequency of each word in the dataset.\n",
    "    \"\"\"\n",
    "    # Tokenize the dataset\n",
    "    words = [word for text in dataset for word in word_tokenize(text)]\n",
    "    # Calculate the frequency\n",
    "    freq = pd.Series(words).value_counts()\n",
    "    return freq\n",
    "\n",
    "def top_k_infrequent(\n",
    "        frequency_table: pd.Series,\n",
    "        query: Iterable[str],\n",
    "        k: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get the k most infrequent words in the dataset.\n",
    "    Only considers words that exist in the table.\n",
    "    Returns AT MOST k words, but no guarantees on the minimum number of words.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for word in query:\n",
    "      if word in frequency_table:\n",
    "        res[word] = frequency_table[word]\n",
    "      #else:\n",
    "      #print(f\"Word not found in frequency table: {word}\")\n",
    "    # Sort items from the dict by value and return list of keys\n",
    "    return [\n",
    "       key\n",
    "       for key, _\n",
    "       in sorted(\n",
    "          res.items(),\n",
    "          key=lambda item: item[1]\n",
    "        )[:k]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_COD(\n",
    "  translation_dataset : pd.DataFrame,\n",
    "  word_dataset : pd.DataFrame,\n",
    "  frequency_dataset : pd.Series,\n",
    "  COD_split : float = 0.5,\n",
    "  COD_words_ratio : float = 0.5, # Ratio of words to be explained\n",
    "  COD_words_max : int = 10, # Maximum number of words to be explained\n",
    "  random_state = 51,  \n",
    "):\n",
    "  # Set random seed\n",
    "  random.seed(random_state)\n",
    "\n",
    "  # Init COD and VANILLA datasets\n",
    "  COD_output = []\n",
    "  VANILLA_output = []\n",
    "\n",
    "  # Shuffle the dataset\n",
    "  input_dataset = translation_dataset.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "  # Loop over translation dataset\n",
    "  offset_vanilla = 0\n",
    "  for i, (index, row) in enumerate(input_dataset.iterrows()):\n",
    "    if i % 1000 == 0:\n",
    "      print(f\"C: Processing {i}th row from {input_dataset.shape[0]} rows\")\n",
    "    # If length/total reached split, break\n",
    "    if len(COD_output) >= COD_split * input_dataset.shape[0]:\n",
    "      # Set the offset for vanilla\n",
    "      offset_vanilla = index\n",
    "      break\n",
    "\n",
    "    # Get the data\n",
    "    ANG_text = row[COL_ANG_TEXT]\n",
    "    EN_text = row[COL_EN_TEXT]\n",
    "\n",
    "    # Search in the word dataset\n",
    "    ANG_words = word_dataset[word_dataset[COL_ANG_TEXT] == ANG_text]\n",
    "\n",
    "    # If not found, skip and push to VANILLA\n",
    "    if len(ANG_words) == 0:\n",
    "      VANILLA_output.append({\n",
    "        COL_ANG_TEXT: ANG_text,\n",
    "        COL_EN_TEXT: EN_text\n",
    "      })\n",
    "      continue\n",
    "\n",
    "    # Get unique words\n",
    "    ANG_words_unique = ANG_words[COL_ANG_WORD].unique()\n",
    "    \n",
    "\n",
    "    # Get the number of words to be explained\n",
    "    n_words = min(COD_words_max, int(COD_words_ratio * len(ANG_words_unique)))\n",
    "    # Select the words by frequency\n",
    "    ANG_words_selected = top_k_infrequent(frequency_dataset, ANG_words_unique, n_words)\n",
    "\n",
    "    # If no words are selected, skip and push to VANILLA\n",
    "    if len(ANG_words_selected) == 0:\n",
    "      VANILLA_output.append({\n",
    "        COL_ANG_TEXT: ANG_text,\n",
    "        COL_EN_TEXT: EN_text\n",
    "      })\n",
    "      continue\n",
    "    \n",
    "    # Build the CoD\n",
    "    explanation_ang_en = \"\"\n",
    "    explanation_en_ang = \"\"\n",
    "    prefix = \"\"\n",
    "    for ANG_word in ANG_words_selected:\n",
    "      # Get the translation in context\n",
    "      EN_word = ANG_words[ANG_words[COL_ANG_WORD] == ANG_word][COL_EN_WORD].values[0]\n",
    "\n",
    "      # Add newline\n",
    "      explanation_ang_en += prefix\n",
    "      explanation_en_ang += prefix\n",
    "      prefix = \" \"\n",
    "\n",
    "      # Append to explanation\n",
    "      explanation_ang_en += f\"\\\"{ANG_word}\\\" means \\\"{EN_word}\\\".\"\n",
    "      explanation_en_ang += f\"\\\"{EN_word}\\\" means \\\"{ANG_word}\\\".\"\n",
    "\n",
    "    # Append to COD_output\n",
    "    COD_output.append({\n",
    "      COL_ANG_TEXT: ANG_text,\n",
    "      COL_EN_TEXT: EN_text,\n",
    "      COL_COD_ANG2EN: explanation_ang_en,\n",
    "      COL_COD_EN2ANG: explanation_en_ang\n",
    "    })\n",
    "\n",
    "  # Init VANILLA_output with the remaining rows\n",
    "  VANILLA_input = input_dataset.iloc[offset_vanilla:]\n",
    "  for i, (index, row) in enumerate(VANILLA_input.iterrows()):\n",
    "    if i % 1000 == 0:\n",
    "      print(f\"V: Processing {i}th row from {VANILLA_input.shape[0]} rows\")\n",
    "    VANILLA_output.append({\n",
    "      COL_ANG_TEXT: row[COL_ANG_TEXT],\n",
    "      COL_EN_TEXT: row[COL_EN_TEXT]\n",
    "    })\n",
    "\n",
    "\n",
    "  # Turn the output into dataframes\n",
    "  COD_output = pd.DataFrame(COD_output)\n",
    "  VANILLA_output = pd.DataFrame(VANILLA_output)\n",
    "\n",
    "  # Return the output\n",
    "  return COD_output, VANILLA_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common words\n",
    "word_frecuency_lookup = word_frecuency(doec_df[\"ANG_text\"])\n",
    "\n",
    "# Enrich using COD\n",
    "translate_cod_df, translate_vanilla_df  = apply_COD(\n",
    "  translate_train_df,\n",
    "  word_translate_df,\n",
    "  word_frecuency_lookup\n",
    ")\n",
    "\n",
    "print(\"========= Translation datasets =========\")\n",
    "getDatasetStats({\"COD\": translate_cod_df, \"Vanilla\": translate_vanilla_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus filtering\n",
    "Due to the nature of the DOEC corpus, it contains a lot of noise and irrelevant information. To filter out the noise, we will use the following criteria:\n",
    "- Remove fragments with less than 5 words.\n",
    "- Remove fragments with less than 20 characters.\n",
    "- Select the remaining fragments between q1 and q3 based on word and character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corpus_filter(corpus: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Removes low quality rows from the corpus\n",
    "  \"\"\"\n",
    "  # Measure initial size\n",
    "  initial_size = corpus.shape[0]\n",
    "\n",
    "  # Remove NaN\n",
    "  corpus = corpus.dropna()\n",
    "\n",
    "  # Drop low quality rows by rules\n",
    "  # (Some rows are too short to be useful)\n",
    "  # - more than 5 words\n",
    "  corpus = corpus[corpus[COL_ANG_TEXT].str.split().str.len() > 5]\n",
    "  # - more than 20 characters\n",
    "  corpus = corpus[corpus[COL_ANG_TEXT].str.len() > 20]\n",
    "\n",
    "  # Drop by quantiles\n",
    "  # Filter out the rows that are outside of the 1st and 3rd quantiles\n",
    "\n",
    "  # Get quantiles\n",
    "  char_length_q1 = corpus[COL_ANG_TEXT].str.len().quantile(0.25)\n",
    "  char_length_q3 = corpus[COL_ANG_TEXT].str.len().quantile(0.75)\n",
    "  word_count_q1 = corpus[COL_ANG_TEXT].str.split().str.len().quantile(0.25)\n",
    "  word_count_q3 = corpus[COL_ANG_TEXT].str.split().str.len().quantile(0.75)\n",
    "\n",
    "  # Filter based on quantiles\n",
    "  corpus = corpus[\n",
    "    # Must be longer than Q1 and shorter than Q3\n",
    "    # Using * 1.0 to convert to float\n",
    "    (corpus[COL_ANG_TEXT].str.len() * 1.0 > char_length_q1) &\n",
    "    (corpus[COL_ANG_TEXT].str.len() * 1.0 < char_length_q3) &\n",
    "    (corpus[COL_ANG_TEXT].str.split().str.len() * 1.0 > word_count_q1) &\n",
    "    (corpus[COL_ANG_TEXT].str.split().str.len() * 1.0 < word_count_q3) \n",
    "  ]\n",
    "\n",
    "  # Measure final size\n",
    "  final_size = corpus.shape[0]\n",
    "\n",
    "  print(f\"Filtered {initial_size - final_size} rows from {initial_size} to {final_size} (- {(initial_size - final_size) / initial_size * 100:.2f}%)\")\n",
    "\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filtered_df = corpus_filter(doec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus splitting\n",
    "\n",
    "The corpus will be used into different steps of training. To avoid the model from memorizing the translations, we will split the corpus into two parts: train and unseen. The train will be used for the pretraining of the model, while the unseen will be used for data augmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the corpus into Train and Unseen\n",
    "train_corpus_df, unseen_corpus_df = split_tt(corpus_filtered_df, train_size=0.5, random_state=RANDOM_SEEDS[\"corpus0\"])\n",
    "# Show stats\n",
    "print(\"=== Corpus Split ===\")\n",
    "getDatasetStats({\"Train\": train_corpus_df, \"Unseen\": unseen_corpus_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show stats after splitting\n",
    "print(\"=== Train dataset after sampling ===\")\n",
    "getDatasetStats({\"Dictionary\": dictionary_df, \"Translate CoD\": translate_cod_df, \"Translate Vanilla\": translate_vanilla_df, \"Corpus\": train_corpus_df}) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "language_codes = {\n",
    "    \"ANG\": \"Anglo-Saxon\",\n",
    "    \"EN\": \"English\"\n",
    "}\n",
    "\n",
    "\n",
    "def display_dataset_sample(title, ds, sample_index=0):\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(f\"Prompt: {ds['prompt'][sample_index]}\")\n",
    "    print(f\"Answer: {ds['answer'][sample_index]}\")\n",
    "    print(f\"Text: {ds['text'][sample_index]}\")\n",
    "    # Assert that the text is the prompt + answer\n",
    "    if ds['text'][sample_index] != ds['prompt'][sample_index] + ds['answer'][sample_index]:\n",
    "        print(\"ERROR: Text is not prompt + answer\")\n",
    "\n",
    "def display_dataset_sample_prompt_only(title, ds, sample_index=0):\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(f\"Prompt: {ds['prompt'][sample_index]}\")\n",
    "\n",
    "\n",
    "def get_translation_prompt_only(src, tgt, text):\n",
    "    target_language = language_codes[tgt]\n",
    "    source_language = language_codes[src]\n",
    "    prompt = f\"[INST]Translate the following {source_language} fragment to {target_language}[/INST]\\n[{src}]{text}[/{src}]\\n[{tgt}]\"\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "def get_definition_prompt(word, definition):\n",
    "    prompt = f\"[INST]What is the English definition of the following word in Anglo-Saxon?[/INST]\\n[ANG]{word}[/ANG]\\n[EN]\"\n",
    "    answer = f\"{definition}[/EN]\"\n",
    "    text = f\"{prompt}{answer}\"\n",
    "    return {\"prompt\": prompt, \"answer\": answer, \"text\": text}\n",
    "\n",
    "def get_translation_prompt(src, tgt, text, translation):\n",
    "    target_language = language_codes[tgt]\n",
    "    source_language = language_codes[src]\n",
    "    prompt = f\"[INST]Translate the following {source_language} fragment to {target_language}[/INST]\\n[{src}]{text}[/{src}]\\n[{tgt}]\"\n",
    "    answer = f\"{translation}[/{tgt}]\"\n",
    "    text = f\"{prompt}{answer}\"\n",
    "    return {\"prompt\": prompt, \"answer\": answer, \"text\": text}\n",
    "\n",
    "def get_cod_prompt(src, tgt, text, translation, cod):\n",
    "    target_language = language_codes[tgt]\n",
    "    source_language = language_codes[src]\n",
    "    prompt = f\"[INST]Translate the following {source_language} fragment to {target_language}[/INST]\\n[{src}]{text}[/{src}]\\n[DICT]{cod}[DICT]\\n[{tgt}]\"\n",
    "    answer = f\"{translation}[/{tgt}]\"\n",
    "    text = f\"{prompt}{answer}\"\n",
    "    return {\"prompt\": prompt, \"answer\": answer, \"text\": text}\n",
    "\n",
    "def get_corpus_prompt(src, text):\n",
    "    source_language = language_codes[src]\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Select half\n",
    "    first_half = \" \".join(words[:len(words)//2])\n",
    "    second_half = \" \".join(words[len(words)//2:])\n",
    "    prompt = f\"[{src}]{first_half} \"\n",
    "    answer = f\"{second_half}[/{src}]\"\n",
    "    text = f\"{prompt}{answer}\"\n",
    "    return {\"prompt\": prompt, \"answer\": answer, \"text\": text}\n",
    "\n",
    "def build_presynth_dataset(\n",
    "    corpus_df: pd.DataFrame,\n",
    "):\n",
    "    ## Format using translation prompt only\n",
    "    corpus_ds = Dataset.from_pandas(corpus_df)\n",
    "    corpus_ds = corpus_ds.map(\n",
    "        lambda x: get_translation_prompt_only(LANG_CODE_ANG, LANG_CODE_EN, x[COL_ANG_TEXT]),\n",
    "        remove_columns=corpus_ds.column_names\n",
    "    )\n",
    "    ## Print stats\n",
    "    # Get the number of rows in each dataset and their % of the total\n",
    "    total_rows = len(corpus_ds)\n",
    "    #Count characters in prompts\n",
    "    total_chars = sum([len(x['prompt']) for x in corpus_ds]) #type: ignore\n",
    "    #Print stats\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Total characters: {total_chars}\")\n",
    "    # Display a sample\n",
    "    display_dataset_sample_prompt_only(\"Presynth\", corpus_ds)\n",
    "    return corpus_ds\n",
    "\n",
    "def build_test_dataset(\n",
    "    parallel_df : pd.DataFrame,\n",
    "):\n",
    "    ## Parallel forward dataset\n",
    "    parallel_ds = Dataset.from_pandas(parallel_df)\n",
    "    parallel_forward_ds = parallel_ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_ANG, LANG_CODE_EN, x[COL_ANG_TEXT], x[COL_EN_TEXT]),\n",
    "        remove_columns=parallel_ds.column_names\n",
    "    )\n",
    "\n",
    "    ## Parallel backward dataset\n",
    "    parallel_backward_ds = parallel_ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_EN, LANG_CODE_ANG, x[COL_EN_TEXT], x[COL_ANG_TEXT]),\n",
    "        remove_columns=parallel_ds.column_names\n",
    "    )\n",
    "\n",
    "    # Res\n",
    "    test_datasets={\n",
    "        \"Forward\": parallel_forward_ds,\n",
    "        \"Backward\": parallel_backward_ds\n",
    "    }\n",
    "\n",
    "    #Display samples\n",
    "    for name, ds in test_datasets.items():\n",
    "        display_dataset_sample(name, ds)\n",
    "\n",
    "    return test_datasets\n",
    "\n",
    "def build_train_dataset(\n",
    "    dictionary_df: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    parallel_vanilla_df: pd.DataFrame,\n",
    "    parallel_cod_df: pd.DataFrame\n",
    "):\n",
    "    ## Dictionary dataset\n",
    "    dictionary_ds = Dataset.from_pandas(dictionary_df)\n",
    "    dictionary_ds = dictionary_ds.map(\n",
    "        lambda x: get_definition_prompt(x[COL_ANG_WORD], x[COL_EN_DEF]),\n",
    "        remove_columns=dictionary_ds.column_names\n",
    "    )\n",
    "\n",
    "    ## Corpus dataset\n",
    "    corpus_ds = Dataset.from_pandas(corpus_df)\n",
    "    corpus_ds = corpus_ds.map(\n",
    "        lambda x: get_corpus_prompt(LANG_CODE_ANG, x[COL_ANG_TEXT]),\n",
    "        remove_columns=corpus_ds.column_names\n",
    "    )\n",
    "\n",
    "    ## Parallel Vanilla dataset\n",
    "    parallel_vanilla_ds = Dataset.from_pandas(parallel_vanilla_df)\n",
    "    parallel_vanilla_forward_ds = parallel_vanilla_ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_ANG, LANG_CODE_EN, x[COL_ANG_TEXT], x[COL_EN_TEXT]),\n",
    "        remove_columns=parallel_vanilla_ds.column_names\n",
    "    )\n",
    "    parallel_vanilla_backward_ds = parallel_vanilla_ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_EN, LANG_CODE_ANG, x[COL_EN_TEXT], x[COL_ANG_TEXT]),\n",
    "        remove_columns=parallel_vanilla_ds.column_names\n",
    "    )\n",
    "\n",
    "    ## Parallel COD dataset\n",
    "    parallel_cod_ds = Dataset.from_pandas(parallel_cod_df)\n",
    "    parallel_cod_forward_ds = parallel_cod_ds.map(\n",
    "        lambda x: get_cod_prompt(LANG_CODE_ANG, LANG_CODE_EN, x[COL_ANG_TEXT], x[COL_EN_TEXT], x[COL_COD_ANG2EN]),\n",
    "        remove_columns=parallel_cod_ds.column_names\n",
    "    )\n",
    "    parallel_cod_backward_ds = parallel_cod_ds.map(\n",
    "        lambda x: get_cod_prompt(LANG_CODE_EN, LANG_CODE_ANG, x[COL_EN_TEXT], x[COL_ANG_TEXT], x[COL_COD_EN2ANG]),\n",
    "        remove_columns=parallel_cod_ds.column_names\n",
    "    )\n",
    "    \n",
    "    ## Print stats\n",
    "    # Get the number of rows in each dataset and their % of the total\n",
    "    train_datasets = {\n",
    "        \"Dictionary\": dictionary_ds,\n",
    "        \"Corpus\": corpus_ds,\n",
    "        \"Parallel Vanilla Forward\": parallel_vanilla_forward_ds,\n",
    "        \"Parallel Vanilla Backward\": parallel_vanilla_backward_ds,\n",
    "        \"Parallel COD Forward\": parallel_cod_forward_ds,\n",
    "        \"Parallel COD Backward\": parallel_cod_backward_ds,\n",
    "    }\n",
    "    #Count rows\n",
    "    total_rows = sum([len(ds) for ds in train_datasets.values()])\n",
    "    #Count characters in prompts\n",
    "    total_chars = 0\n",
    "    for ds in train_datasets.values():\n",
    "        total_chars += sum([len(x['prompt']) for x in ds]) #type: ignore\t\n",
    "    #Print stats\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Total characters: {total_chars}\")\n",
    "    for name, ds in train_datasets.items():\n",
    "        print(f\"{name}: {len(ds)} ({len(ds) / total_rows * 100:.2f}%)\")\n",
    "        current_chars = sum([len(x['prompt']) for x in ds]) #type: ignore\t\n",
    "        print(f\"  - Characters: {current_chars} ({current_chars / total_chars * 100:.2f}%)\")\n",
    "\n",
    "    # Print a sample of each dataset\n",
    "    for name, ds in train_datasets.items():\n",
    "        display_dataset_sample(name, ds)\n",
    "\n",
    "    # v1: Parallel + Corpus\n",
    "    train_ds_v1 = concatenate_datasets(\n",
    "        [train_datasets[ds] for ds in train_datasets if \"Dictionary\" not in ds]\n",
    "    )\n",
    "    train_ds_v1 = train_ds_v1.shuffle(seed=42)\n",
    "\n",
    "    # v2: Parallel + Corpus + Dictionary\n",
    "    train_ds_v2 = concatenate_datasets(\n",
    "        [train_datasets[ds] for ds in train_datasets]\n",
    "    )\n",
    "    train_ds_v2 = train_ds_v2.shuffle(seed=42)\n",
    "\n",
    "    return {\n",
    "        \"v1\": train_ds_v1,\n",
    "        \"v2\": train_ds_v2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = build_train_dataset(\n",
    "  dictionary_df,\n",
    "  train_corpus_df,\n",
    "  translate_vanilla_df,\n",
    "  translate_cod_df\n",
    ")\n",
    "presynth_dataset = build_presynth_dataset(\n",
    "  unseen_corpus_df\n",
    ")\n",
    "test_datasets = build_test_dataset(\n",
    "  translate_test_df\n",
    ")\n",
    "val_datasets = build_test_dataset(\n",
    "  translate_val_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create repository\n",
    "api = HfApi()\n",
    "repo_url = api.create_repo(repo_id=HF_REPO,private=True,exist_ok=True)\n",
    "# Upload readme (init)\n",
    "api.upload_file(path_or_fileobj=\"README.md\", path_in_repo=\"README.md\",      repo_id=HF_REPO, commit_message=\"Add README.md\")\n",
    "# Create repository object\n",
    "repo = Repository(local_dir=\"hf-repo\", clone_from=repo_url)\n",
    "\n",
    "# Save datasets\n",
    "## Train\n",
    "train_datasets[\"v1\"].save_to_disk(\"hf-repo/train_v1\")\n",
    "train_datasets[\"v2\"].save_to_disk(\"hf-repo/train_v2\")\n",
    "## Pre-Synthetic\n",
    "presynth_dataset.save_to_disk(\"hf-repo/presynth\")\n",
    "## Test\n",
    "test_datasets[\"Forward\"].save_to_disk(\"hf-repo/test_forward\")\n",
    "test_datasets[\"Backward\"].save_to_disk(\"hf-repo/test_backward\")\n",
    "## Val\n",
    "val_datasets[\"Forward\"].save_to_disk(\"hf-repo/val_forward\")\n",
    "val_datasets[\"Backward\"].save_to_disk(\"hf-repo/val_backward\")\n",
    "## Push to repo\n",
    "repo.push_to_hub(commit_message=\"Add datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
