{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from huggingface_hub import notebook_login, Repository \n",
    "from datasets import load_from_disk\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "HF_BASE_MODEL_ID = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "HF_MODEL_ID = \"RodrigoSalazar-U/ang-base\"\n",
    "HF_MODEL_CHECKPOINT_ID = HF_MODEL_ID+\"-checkpoints\"\n",
    "\n",
    "# Dataset\n",
    "DATASET_PATH = \"./hf-repo/train\"\n",
    "\n",
    "# Training \n",
    "## General config\n",
    "TRAIN_LR = 2*5e-5\n",
    "TRAIN_EMBEDDING_LR = TRAIN_LR / 2\n",
    "TRAIN_WARMUP_RATIO = 0.1\n",
    "TRAIN_EPOCHS = 5\n",
    "TRAIN_OPTIMIZER = \"adamw_8bit\"\n",
    "TRAIN_WEIGHT_DECAY = 0.00\n",
    "TRAIN_LR_SCHEDULER = \"cosine\"\n",
    "TRAIN_SEED = 512\n",
    "\n",
    "## Batching config\n",
    "BATCH_PER_DEVICE = 16\n",
    "GRADIENT_ACCUMULATION = 2 \n",
    "# Effective batch size= BATCH_SIZE_PER_DEVICE * GRADIENT_ACCUMULATION = 32\n",
    "\n",
    "## Lora config\n",
    "ADAPTER_R     = 256\n",
    "ADAPTER_ALPHA = 32\n",
    "ADAPTER_SEED  = 3407\n",
    "ADAPTER_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # Attention\n",
    "                  \"gate_proj\", \"up_proj\", \"down_proj\", # Gate+Up+Down Proj\n",
    "                  \"embed_tokens\", \"lm_head\",] # Embedding + LM head for CPT\n",
    "ADAPTER_RSLORA = True\n",
    "\n",
    "## Model config\n",
    "MODEL_MAX_SEQ_LENGTH = 2048\n",
    "MODEL_DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "MODEL_LOAD_IN_4BIT = True # Use 4bit quantization to reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounts login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging to wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "  project=f\"LLM-EN2ANG\",\n",
    "  entity=\"rodrigo-salazar-utec\",\n",
    "  name=HF_MODEL_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Download base model and initialize using unslothed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Loading model {HF_BASE_MODEL_ID}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = HF_BASE_MODEL_ID,\n",
    "    max_seq_length = MODEL_MAX_SEQ_LENGTH,\n",
    "    dtype = MODEL_DTYPE,\n",
    "    load_in_4bit = MODEL_LOAD_IN_4BIT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = ADAPTER_R,\n",
    "    target_modules = ADAPTER_TARGET_MODULES,\n",
    "    lora_alpha = ADAPTER_ALPHA,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = ADAPTER_SEED, # Random seed for repro\n",
    "    use_rslora = ADAPTER_RSLORA,  # Rank stabilized LoRA\n",
    "    loftq_config = None, # LoftQ disabled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Expects HF datasets file.\n",
    "Format of the dataset is as follows:\n",
    "- `prompt`: the prompt for the task\n",
    "- `answer`: the answer for the task\n",
    "- `text`: full text. should be equal to `prompt` + `answer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Select only prompt, answer and text columns\n",
    "columns = [\"prompt\", \"answer\", \"text\"]\n",
    "removed_columns = list(set(dataset.column_names) - set(columns))\n",
    "dataset = dataset.remove_columns(removed_columns)\n",
    "\n",
    "# Append EOS token to text\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def append_eos_token(examples):\n",
    "  text = examples[\"text\"] + EOS_TOKEN\n",
    "  return {\"text\": text}\n",
    "dataset = dataset.map(append_eos_token)\n",
    "\n",
    "# Display dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MODEL_MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = BATCH_PER_DEVICE, #2,\n",
    "        gradient_accumulation_steps = GRADIENT_ACCUMULATION, #64,\n",
    "        warmup_ratio = TRAIN_WARMUP_RATIO,\n",
    "        num_train_epochs = TRAIN_EPOCHS,\n",
    "        learning_rate = TRAIN_LR,\n",
    "        embedding_learning_rate = TRAIN_EMBEDDING_LR,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = TRAIN_OPTIMIZER,\n",
    "        weight_decay = TRAIN_WEIGHT_DECAY,\n",
    "        lr_scheduler_type = TRAIN_LR_SCHEDULER,\n",
    "        seed = TRAIN_SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        #W&B\n",
    "        run_name = HF_MODEL_ID,\n",
    "        report_to = \"wandb\",\n",
    "        # Checkpoints\n",
    "        save_steps = 500,\n",
    "        save_total_limit = 3,\n",
    "        push_to_hub = True,\n",
    "        hub_model_id = HF_MODEL_CHECKPOINT_ID,\n",
    "        hub_strategy = \"checkpoint\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push merged model\n",
    "model.push_to_hub_merged(HF_MODEL_ID, tokenizer=tokenizer, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Colab runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
