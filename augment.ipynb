{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from huggingface_hub import notebook_login \n",
    "from datasets import load_from_disk, Dataset, concatenate_datasets\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "HF_MODEL_ID = \"RodrigoSalazar-U/ang-base\"\n",
    "\n",
    "# Dataset\n",
    "INPUT_DATASET_PATH = \"./hf-repo/unseen\"\n",
    "OUTPUT_DATASET_PATH = \"./hf-repo/augmented\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounts login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Download base model and initialize using unslothed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model {HF_MODEL_ID}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = HF_MODEL_ID,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Expects HF datasets file.\n",
    "Format of the dataset is as follows:\n",
    "- `prompt`: the prompt for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "input_dataset = load_from_disk(INPUT_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TAG = \"[/\"\n",
    "\n",
    "## Function to generate translations of a list of input prompts\n",
    "def generate_translation(inp_prompts):\n",
    "    # Tokenize input prompts and move to GPU\n",
    "    inputs = tokenizer(\n",
    "        inp_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Ensure batch processing works\n",
    "        truncation=True  # In case any prompt exceeds max length\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output for each prompt in the list\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        min_new_tokens=3, # Force generation\n",
    "        stop_strings=[END_TAG],  # Use list for stop strings to handle multiple inputs\n",
    "        tokenizer=tokenizer,\n",
    "        use_cache=True,\n",
    "        do_sample=False,     # Deterministic\n",
    "        #num_beam = 5,       # Number of beams\n",
    "    )\n",
    "\n",
    "    # Decode batch of outputs\n",
    "    generated_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process each output\n",
    "    final_outputs = [g for g in generated_outputs]\n",
    "\n",
    "    # Move tensors back to CPU and clear GPU memory\n",
    "    inputs = inputs.to(\"cpu\")\n",
    "    outputs = outputs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Return list of generated outputs\n",
    "    return final_outputs\n",
    "\n",
    "## Batch generation of translations for a dataset\n",
    "def generate_translation_dataset(dataset: Dataset, batch_size: int = 32) -> Dataset:\n",
    "    translation_pairs = []\n",
    "\n",
    "    # Iterate over the dataset in batches\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        # Collect the input prompts for the current batch\n",
    "        inp_batch = dataset[\"prompt\"][i:i + batch_size]\n",
    "\n",
    "        # Generate translations for the batch of inputs\n",
    "        generated_batch = generate_translation(inp_batch)\n",
    "\n",
    "        # Append each translation pair (expected, generated) to the list\n",
    "        for prompt, generated in zip(inp_batch, generated_batch):\n",
    "            translation_pairs.append({\"prompt\": prompt, \"generated\": generated})\n",
    "\n",
    "    # Convert the list of translation pairs back into a Dataset\n",
    "    return Dataset.from_list(translation_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = generate_translation_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "Remove any low quality outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_ANG_TEXT = \"ANG_text\"\n",
    "COL_EN_TEXT = \"EN_text\"\n",
    "LANG_CODE_ANG = \"ANG\"\n",
    "LANG_CODE_EN = \"EN\"\n",
    "\n",
    "language_codes = {\n",
    "    \"ANG\": \"Anglo-Saxon\",\n",
    "    \"EN\": \"English\"\n",
    "}\n",
    "\n",
    "def get_translation_prompt(src, tgt, text, translation):\n",
    "    target_language = language_codes[tgt]\n",
    "    source_language = language_codes[src]\n",
    "    prompt = f\"[INST]Translate the following {source_language} fragment to {target_language}[/INST]\\n[{src}]{text}[/{src}]\\n[{tgt}]\"\n",
    "    answer = f\"{translation}[/{tgt}]\"\n",
    "    text = f\"{prompt}{answer}\"\n",
    "    return {\"prompt\": prompt, \"answer\": answer, \"text\": text}\n",
    "\n",
    "def synthetic_filter(\n",
    "    synth: pd.DataFrame,\n",
    "    min_generation_length: int = 20,\n",
    "    min_word_count: int = 5,\n",
    "    max_generation_length: int = 1000,\n",
    "    # 1024 in reality. slightly less to be safe\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Removes low quality rows from the synthetic parallel corpus\n",
    "  \"\"\"\n",
    "  # Measure initial size\n",
    "  initial_size = synth.shape[0]\n",
    "  print(f\"Initial size: {initial_size}\")\n",
    "\n",
    "  # Remove NaN\n",
    "  synth = synth.dropna()\n",
    "\n",
    "  # Validate the length of the generated text\n",
    "  synth = synth[synth[COL_EN_TEXT].str.len() > min_generation_length]\n",
    "  synth = synth[synth[COL_EN_TEXT].str.len() < max_generation_length]\n",
    "\n",
    "  # Minimum word count\n",
    "  synth = synth[synth[COL_EN_TEXT].str.split().str.len() > min_word_count]\n",
    "\n",
    "  # Detect if EN column has ANG exclusive characters\n",
    "  def has_ang_chars(row):\n",
    "    for char in row[COL_EN_TEXT]:\n",
    "      if char in \"ÆæǷƿÞþÐð\":\n",
    "        return True\n",
    "    return False\n",
    "  synth['has_ang_chars'] = synth.apply(has_ang_chars, axis=1)\n",
    "\n",
    "  # Remove rows with ANG exclusive characters\n",
    "  synth = synth[synth['has_ang_chars'] == False]\n",
    "\n",
    "  # Drop the has_ang_chars column\n",
    "  synth = synth.drop(columns=['has_ang_chars'])\n",
    "\n",
    "  # Get final size\n",
    "  final_size = synth.shape[0]\n",
    "  print(f\"Filtered {initial_size - final_size} rows from {initial_size} to {final_size} (- {(initial_size - final_size) / initial_size * 100:.2f}%)\")\n",
    "\n",
    "  return synth\n",
    "\n",
    "def build_train_from_synth(input_dataset,  random_seed=751):\n",
    "    print(f\"Raw dataset: {len(input_dataset)} rows\")\n",
    "    input_len = len(input_dataset)\n",
    "\n",
    "    # Extract EN and ANG sections from the dataset\n",
    "    data = []\n",
    "    for row in input_dataset:\n",
    "        text = row['text']\n",
    "        # If the text does not end with \"[/\" then it is not a valid entry\n",
    "        if not text.endswith(\"[/\"):\n",
    "            continue\n",
    "\n",
    "        # Capture [EN]text[/EN] and [ANG]text[/\n",
    "        en_groups = re.findall(r'\\[EN\\](.*?)\\[/', text)\n",
    "        ang_groups = re.findall(r'\\[ANG\\](.*?)\\[/ANG\\]', text)\n",
    "        if len(en_groups) == 0 or len(ang_groups) == 0:\n",
    "            # Skip if no match\n",
    "            continue\n",
    "        en_text = en_groups[0].strip()\n",
    "        ang_text = ang_groups[0].strip()\n",
    "\n",
    "        # Append to the dataset\n",
    "        data.append({COL_ANG_TEXT: ang_text, COL_EN_TEXT: en_text})\n",
    "    \n",
    "    extracted_len = len(data)\n",
    "    # Display stats\n",
    "    print(f\"Miss generated: {input_len - extracted_len} rows ({(input_len - extracted_len) / input_len * 100:.2f}%)\")\n",
    "\n",
    "\n",
    "    # Create a dataframe\n",
    "    synth_df = pd.DataFrame(data)\n",
    "    # Apply quality filter\n",
    "    synth_df = synthetic_filter(synth_df)\n",
    "\n",
    "    # Create a dataset\n",
    "    ds = Dataset.from_pandas(synth_df)\n",
    "    # Create the prompt\n",
    "    ds_forward = ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_ANG, LANG_CODE_EN, x[COL_ANG_TEXT], x[COL_EN_TEXT]),\n",
    "        remove_columns=ds.column_names\n",
    "    )\n",
    "    ds_backward = ds.map(\n",
    "        lambda x: get_translation_prompt(LANG_CODE_EN, LANG_CODE_ANG, x[COL_EN_TEXT], x[COL_ANG_TEXT]),\n",
    "        remove_columns=ds.column_names\n",
    "    )\n",
    "    # Return combined and shuffled dataset\n",
    "    return concatenate_datasets([ds_forward, ds_backward]).shuffle(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the augmented dataset\n",
    "output_dataset = build_train_from_synth(synthetic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Colab runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
